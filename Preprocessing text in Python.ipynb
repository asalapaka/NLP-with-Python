{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will preprocess text using an approach called bag-of-words where each text is represented by its words regardless of the order in which they are presented or the embedded grammar by completing the following steps:\n",
    "\n",
    " 1. Tokenise \n",
    " 2. Normalise \n",
    " 3. Remove stop words\n",
    " 4. Count vectorise\n",
    " 5. Transform to tf-idf representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up the Python environment\n",
    "#### Download ‘stopwords’ and ‘wordnet’ corpora from nltk\n",
    "The script below can help you download these corpora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/adarshsalapaka/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/adarshsalapaka/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords') \n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "We will use tiny text data which will allow us to monitor inputs and outputs for each step. \n",
    "\n",
    "**For this data, I have chosen bold, anarchistic freedom fighter V(Hugo Weaving)'s speech when he took over the state television broadcast a day after destroying the Old Bailey. He offered the people of London in the UK reasons to ignite a revolution against oppressive, totalitarian (fascist) government. He urged the people of Britain to rise up and meet him on November 5th one year later outside the gates of Parliament, which he promised would also be destroyed**\n",
    "\n",
    "\n",
    "His speech goes: Good evening, London. Allow me first to apologize for this interruption. I do, like many of you, appreciate the comforts of every day routine - the security of the familiar, the tranquility of repetition. I enjoy them as much as any bloke. But in the spirit of commemoration, thereby those important events of the past usually associated with someone's death or the end of some awful bloody struggle, a celebration of a nice holiday, I thought we could mark this November the 5th, a day that is sadly no longer remembered, by taking some time out of our daily lives to sit down and have a little chat.\n",
    "\n",
    "There are of course those who do not want us to speak. I suspect even now, orders are being shouted into telephones, and men with guns will soon be on their way. Why? Because while the truncheon may be used in lieu of conversation, words will always retain their power. Words offer the means to meaning, and for those who will listen, the enunciation of truth. And the truth is, there is something terribly wrong with this country, isn't there?\n",
    "\n",
    "Cruelty and injustice, intolerance and oppression. And where once you had the freedom to object, to think and speak as you saw fit, you now have censors and systems of surveillance coercing your conformity and soliciting your submission. How did this happen? Who's to blame? Well certainly there are those who are more responsible than others, and they will be held accountable, but again truth be told, if you're looking for the guilty, you need only look into a mirror.\n",
    "\n",
    "I know why you did it. I know you were afraid. Who wouldn't be? War, terror, disease. There were a myriad of problems which conspired to corrupt your reason and rob you of your common sense. Fear got the best of you, and in your panic you turned to the now high chancellor, Adam Sutler. He promised you order, he promised you peace, and all he demanded in return was your silent, obedient consent. Last night, I sought to end that silence. Last night, I destroyed the Old Bailey, to remind this country of what it has forgotten. More than four hundred years ago, a great citizen wished to embed the 5th of November forever in our memory. His hope was to remind the world that fairness, justice, and freedom are more than words - they are perspectives. So if you've seen nothing, if the crimes of this government remain unknown to you, then I would suggest you allow the 5th of November to pass unmarked.\n",
    "\n",
    "But if you see what I see, if you feel as I feel, and if you would seek as I seek, then I ask you to stand beside me one year from tonight, outside the gates of Parliament, and together we shall give them a 5th of November that shall never, ever be forgot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "part1 = \"\"\"Good evening, London. Allow me first to apologize for this interruption. I do, like many of you, appreciate the comforts of every day routine - the security of the familiar, the tranquility of repetition. I enjoy them as much as any bloke. But in the spirit of commemoration, thereby those important events of the past usually associated with someone's death or the end of some awful bloody struggle, a celebration of a nice holiday, I thought we could mark this November the 5th, a day that is sadly no longer remembered, by taking some time out of our daily lives to sit down and have a little chat.\n",
    "\n",
    "There are of course those who do not want us to speak. I suspect even now, orders are being shouted into telephones, and men with guns will soon be on their way. Why? Because while the truncheon may be used in lieu of conversation, words will always retain their power. Words offer the means to meaning, and for those who will listen, the enunciation of truth. And the truth is, there is something terribly wrong with this country, isn't there?\n",
    "\n",
    "Cruelty and injustice, intolerance and oppression. And where once you had the freedom to object, to think and speak as you saw fit, you now have censors and systems of surveillance coercing your conformity and soliciting your submission. How did this happen? Who's to blame? Well certainly there are those who are more responsible than others, and they will be held accountable, but again truth be told, if you're looking for the guilty, you need only look into a mirror.\"\"\"\n",
    "\n",
    "part2 = \"\"\"I know why you did it. I know you were afraid. Who wouldn't be? War, terror, disease. There were a myriad of problems which conspired to corrupt your reason and rob you of your common sense. Fear got the best of you, and in your panic you turned to the now high chancellor, Adam Sutler. He promised you order, he promised you peace, and all he demanded in return was your silent, obedient consent. Last night, I sought to end that silence. Last night, I destroyed the Old Bailey, to remind this country of what it has forgotten. More than four hundred years ago, a great citizen wished to embed the 5th of November forever in our memory. His hope was to remind the world that fairness, justice, and freedom are more than words - they are perspectives. So if you've seen nothing, if the crimes of this government remain unknown to you, then I would suggest you allow the 5th of November to pass unmarked.\n",
    "\n",
    "But if you see what I see, if you feel as I feel, and if you would seek as I seek, then I ask you to stand beside me one year from tonight, outside the gates of Parliament, and together we shall give them a 5th of November that shall never, ever be forgot.\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare the packages to analyze data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages and modules\n",
    "import pandas as pd\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "X_train = pd.DataFrame([part1, part2], columns=['speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, let’s define a text preprocessing function to pass it on to TfidfVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # Tokenise words while ignoring punctuation\n",
    "    tokeniser = RegexpTokenizer(r'\\w+')\n",
    "    tokens = tokeniser.tokenize(text)\n",
    "    \n",
    "    # Lowercase and lemmatise \n",
    "    lemmatiser = WordNetLemmatizer()\n",
    "    lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    keywords= [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let’s preprocess the text data by leveraging the function defined earlier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5th</th>\n",
       "      <th>accountable</th>\n",
       "      <th>adam</th>\n",
       "      <th>afraid</th>\n",
       "      <th>ago</th>\n",
       "      <th>allow</th>\n",
       "      <th>always</th>\n",
       "      <th>apologize</th>\n",
       "      <th>appreciate</th>\n",
       "      <th>ask</th>\n",
       "      <th>...</th>\n",
       "      <th>war</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wish</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.061661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.061661</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.173324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.086662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        5th  accountable      adam    afraid       ago     allow    always  \\\n",
       "0  0.061335     0.086204  0.000000  0.000000  0.000000  0.061335  0.086204   \n",
       "1  0.184983     0.000000  0.086662  0.086662  0.086662  0.061661  0.000000   \n",
       "\n",
       "   apologize  appreciate       ask  ...       war       way      well  \\\n",
       "0   0.086204    0.086204  0.000000  ...  0.000000  0.086204  0.086204   \n",
       "1   0.000000    0.000000  0.086662  ...  0.086662  0.000000  0.000000   \n",
       "\n",
       "       wish      word     world     would     wrong      year     years  \n",
       "0  0.000000  0.122670  0.000000  0.000000  0.086204  0.000000  0.000000  \n",
       "1  0.086662  0.061661  0.086662  0.173324  0.000000  0.086662  0.086662  \n",
       "\n",
       "[2 rows x 189 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create an instance of TfidfVectorizer\n",
    "vectoriser = TfidfVectorizer(analyzer=preprocess_text)\n",
    "\n",
    "# Fit to the data and transform to feature matrix\n",
    "X_train = vectoriser.fit_transform(X_train['speech'])\n",
    "\n",
    "# Convert sparse matrix to dataframe\n",
    "X_train = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "\n",
    "# Save mapping on which index refers to which words\n",
    "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
    "\n",
    "# Rename each column using the mapping\n",
    "for col in X_train.columns:\n",
    "    X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
    "    \n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have preprocessed text into feature matrix. Let’s break this down and understand the 5 steps mentioned at the beginning with examples in the next section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Code Explanation\n",
    "\n",
    "#### 1. Tokenization\n",
    "\n",
    "**Given a character sequence and a defined document unit, tokenization is the task of chopping it up into pieces, called tokens , perhaps at the same time throwing away certain characters, such as punctuation.**\n",
    "\n",
    "\n",
    "In this step, we will convert a string part1 into list of tokens while discarding punctuation. There are many ways we could accomplish this task. I will show you one way to do so by using RegexpTokenizer from nltk:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Good', 'evening', 'London', 'Allow', 'me', 'first', 'to', 'apologize', 'for', 'this', 'interruption', 'I', 'do', 'like', 'many', 'of', 'you', 'appreciate', 'the', 'comforts', 'of', 'every', 'day', 'routine', 'the', 'security', 'of', 'the', 'familiar', 'the', 'tranquility', 'of', 'repetition', 'I', 'enjoy', 'them', 'as', 'much', 'as', 'any', 'bloke', 'But', 'in', 'the', 'spirit', 'of', 'commemoration', 'thereby', 'those', 'important', 'events', 'of', 'the', 'past', 'usually', 'associated', 'with', 'someone', 's', 'death', 'or', 'the', 'end', 'of', 'some', 'awful', 'bloody', 'struggle', 'a', 'celebration', 'of', 'a', 'nice', 'holiday', 'I', 'thought', 'we', 'could', 'mark', 'this', 'November', 'the', '5th', 'a', 'day', 'that', 'is', 'sadly', 'no', 'longer', 'remembered', 'by', 'taking', 'some', 'time', 'out', 'of', 'our', 'daily', 'lives', 'to', 'sit', 'down', 'and', 'have', 'a', 'little', 'chat', 'There', 'are', 'of', 'course', 'those', 'who', 'do', 'not', 'want', 'us', 'to', 'speak', 'I', 'suspect', 'even', 'now', 'orders', 'are', 'being', 'shouted', 'into', 'telephones', 'and', 'men', 'with', 'guns', 'will', 'soon', 'be', 'on', 'their', 'way', 'Why', 'Because', 'while', 'the', 'truncheon', 'may', 'be', 'used', 'in', 'lieu', 'of', 'conversation', 'words', 'will', 'always', 'retain', 'their', 'power', 'Words', 'offer', 'the', 'means', 'to', 'meaning', 'and', 'for', 'those', 'who', 'will', 'listen', 'the', 'enunciation', 'of', 'truth', 'And', 'the', 'truth', 'is', 'there', 'is', 'something', 'terribly', 'wrong', 'with', 'this', 'country', 'isn', 't', 'there', 'Cruelty', 'and', 'injustice', 'intolerance', 'and', 'oppression', 'And', 'where', 'once', 'you', 'had', 'the', 'freedom', 'to', 'object', 'to', 'think', 'and', 'speak', 'as', 'you', 'saw', 'fit', 'you', 'now', 'have', 'censors', 'and', 'systems', 'of', 'surveillance', 'coercing', 'your', 'conformity', 'and', 'soliciting', 'your', 'submission', 'How', 'did', 'this', 'happen', 'Who', 's', 'to', 'blame', 'Well', 'certainly', 'there', 'are', 'those', 'who', 'are', 'more', 'responsible', 'than', 'others', 'and', 'they', 'will', 'be', 'held', 'accountable', 'but', 'again', 'truth', 'be', 'told', 'if', 'you', 're', 'looking', 'for', 'the', 'guilty', 'you', 'need', 'only', 'look', 'into', 'a', 'mirror']\n"
     ]
    }
   ],
   "source": [
    "# Import module\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "\n",
    "# Create an instance of RegexpTokenizer for alphanumeric tokens\n",
    "tokeniser = RegexpTokenizer(r'\\w+')\n",
    "\n",
    "# Tokenise 'part1' string\n",
    "tokens = tokeniser.tokenize(part1)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that each word is now a separate string. Do you notice how there are variations of the same word? \n",
    "\n",
    "For instance: words can differ in terms of their case: ‘and’ and ‘And’ or their suffix: ‘share’, ‘shared’ and ‘sharing’. This is where normalisation comes in to standardise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Normalize\n",
    "\n",
    "**To normalise a word is to transform it into its root form**\n",
    "\n",
    "Stemming and lemmatisation are popular ways to normalise text. In this step, we will use lemmatisation to transform words to their dictionary form as well as remove case distinction by converting all words to lowercase.\n",
    "\n",
    "We will use WordNetLemmatizer from nltk to lemmatise our tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'even', 'london', 'allow', 'me', 'first', 'to', 'apologize', 'for', 'this', 'interruption', 'i', 'do', 'like', 'many', 'of', 'you', 'appreciate', 'the', 'comfort', 'of', 'every', 'day', 'routine', 'the', 'security', 'of', 'the', 'familiar', 'the', 'tranquility', 'of', 'repetition', 'i', 'enjoy', 'them', 'as', 'much', 'as', 'any', 'bloke', 'but', 'in', 'the', 'spirit', 'of', 'commemoration', 'thereby', 'those', 'important', 'events', 'of', 'the', 'past', 'usually', 'associate', 'with', 'someone', 's', 'death', 'or', 'the', 'end', 'of', 'some', 'awful', 'bloody', 'struggle', 'a', 'celebration', 'of', 'a', 'nice', 'holiday', 'i', 'think', 'we', 'could', 'mark', 'this', 'november', 'the', '5th', 'a', 'day', 'that', 'be', 'sadly', 'no', 'longer', 'remember', 'by', 'take', 'some', 'time', 'out', 'of', 'our', 'daily', 'live', 'to', 'sit', 'down', 'and', 'have', 'a', 'little', 'chat', 'there', 'be', 'of', 'course', 'those', 'who', 'do', 'not', 'want', 'us', 'to', 'speak', 'i', 'suspect', 'even', 'now', 'order', 'be', 'be', 'shout', 'into', 'telephone', 'and', 'men', 'with', 'gun', 'will', 'soon', 'be', 'on', 'their', 'way', 'why', 'because', 'while', 'the', 'truncheon', 'may', 'be', 'use', 'in', 'lieu', 'of', 'conversation', 'word', 'will', 'always', 'retain', 'their', 'power', 'word', 'offer', 'the', 'mean', 'to', 'mean', 'and', 'for', 'those', 'who', 'will', 'listen', 'the', 'enunciation', 'of', 'truth', 'and', 'the', 'truth', 'be', 'there', 'be', 'something', 'terribly', 'wrong', 'with', 'this', 'country', 'isn', 't', 'there', 'cruelty', 'and', 'injustice', 'intolerance', 'and', 'oppression', 'and', 'where', 'once', 'you', 'have', 'the', 'freedom', 'to', 'object', 'to', 'think', 'and', 'speak', 'as', 'you', 'saw', 'fit', 'you', 'now', 'have', 'censor', 'and', 'systems', 'of', 'surveillance', 'coerce', 'your', 'conformity', 'and', 'solicit', 'your', 'submission', 'how', 'do', 'this', 'happen', 'who', 's', 'to', 'blame', 'well', 'certainly', 'there', 'be', 'those', 'who', 'be', 'more', 'responsible', 'than', 'others', 'and', 'they', 'will', 'be', 'hold', 'accountable', 'but', 'again', 'truth', 'be', 'tell', 'if', 'you', 're', 'look', 'for', 'the', 'guilty', 'you', 'need', 'only', 'look', 'into', 'a', 'mirror']\n"
     ]
    }
   ],
   "source": [
    "# Import module\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Create an instance of WordNetLemmatizer\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "# Lowercase and lemmatise tokens\n",
    "lemmas = [lemmatiser.lemmatize(token.lower(), pos='v') for token in tokens]\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The words are now transformed to its dictionary form. For instance, ‘share’, ‘sharing’ and ‘shared’ are now all just ‘share’."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "271"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many words we have\n",
    "len(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 271 words but not all words carry the same level of contribution to the meaning of the text. In other words, there are some words that are not particularly useful to the key message. This is where stop words come in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Removing stop words\n",
    "\n",
    "**Stop words are common words which provide little to no value to the meaning of the text.**\n",
    "\n",
    "Think about this: If you had to describe yourself in three words as elaborately as possible, would you include ‘I’, or ‘am’? If I asked you to underline keywords in Joey’s speech, would you underline ‘a’ or ‘the’? Probably not. The ‘I’, ‘am’, ‘a’ and ‘the’ are examples of stop words. I think you get the idea.\n",
    "\n",
    "Different sets of stop words may be necessary depending on the domain that the text is related to. In this step, we will leverage nltk’s stopwords corpus. You could define your own set of stop words or enrich standard stop words by adding common terms that are appropriate to the domain of the text.\n",
    "\n",
    "Let’s first familiarise ourselves with stopwords little bit more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "179\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['i', 'me', 'my', 'myself', 'we']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import module\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Check out how many stop words there are \n",
    "print(len(stopwords.words('english')))\n",
    "\n",
    "# See first 5 stop words\n",
    "stopwords.words('english')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the time of writing this post, there are 179 english stop words in the nltk’s stopword corpus.\n",
    "\n",
    "Some examples include: ‘i’, ‘me’, ‘my’, ‘myself’, ‘we’. If you are curious to see the full list, simply remove [:5] from the last line of code.\n",
    "\n",
    "Notice how these stop words are in lowercase? To effectively remove stop words, we have to ensure that all words are in lowercase. Here, we have already done so in step two.\n",
    "\n",
    "Using a list comprehension, let’s remove all stop words from our list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['good', 'even', 'london', 'allow', 'first', 'apologize', 'interruption', 'like', 'many', 'appreciate', 'comfort', 'every', 'day', 'routine', 'security', 'familiar', 'tranquility', 'repetition', 'enjoy', 'much', 'bloke', 'spirit', 'commemoration', 'thereby', 'important', 'events', 'past', 'usually', 'associate', 'someone', 'death', 'end', 'awful', 'bloody', 'struggle', 'celebration', 'nice', 'holiday', 'think', 'could', 'mark', 'november', '5th', 'day', 'sadly', 'longer', 'remember', 'take', 'time', 'daily', 'live', 'sit', 'little', 'chat', 'course', 'want', 'us', 'speak', 'suspect', 'even', 'order', 'shout', 'telephone', 'men', 'gun', 'soon', 'way', 'truncheon', 'may', 'use', 'lieu', 'conversation', 'word', 'always', 'retain', 'power', 'word', 'offer', 'mean', 'mean', 'listen', 'enunciation', 'truth', 'truth', 'something', 'terribly', 'wrong', 'country', 'cruelty', 'injustice', 'intolerance', 'oppression', 'freedom', 'object', 'think', 'speak', 'saw', 'fit', 'censor', 'systems', 'surveillance', 'coerce', 'conformity', 'solicit', 'submission', 'happen', 'blame', 'well', 'certainly', 'responsible', 'others', 'hold', 'accountable', 'truth', 'tell', 'look', 'guilty', 'need', 'look', 'mirror']\n"
     ]
    }
   ],
   "source": [
    "keywords = [lemma for lemma in lemmas if lemma not in stopwords.words('english')]\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check how many words we have\n",
    "len(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removing stop words, we only have 120 words as opposed to 271 yet the gist is still preserved.\n",
    "\n",
    "Now, if you scroll back up to section 2 (Final Code) and have a quick look at the preprocess_text function, you will see that this function captures the transformation process shown in steps 1 to 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Count Vectorize\n",
    "\n",
    "**Count vectorise is to convert a collection of text documents to a matrix of token counts.**\n",
    "\n",
    "Now let’s look at counts of each word in keywords from step 3:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sit': 1,\n",
       " 'celebration': 1,\n",
       " 'listen': 1,\n",
       " 'suspect': 1,\n",
       " 'use': 1,\n",
       " 'remember': 1,\n",
       " 'telephone': 1,\n",
       " 'way': 1,\n",
       " 'injustice': 1,\n",
       " 'sadly': 1,\n",
       " 'men': 1,\n",
       " 'terribly': 1,\n",
       " 'hold': 1,\n",
       " 'bloody': 1,\n",
       " 'security': 1,\n",
       " 'want': 1,\n",
       " 'past': 1,\n",
       " 'conversation': 1,\n",
       " 'well': 1,\n",
       " 'first': 1,\n",
       " 'mark': 1,\n",
       " 'november': 1,\n",
       " 'coerce': 1,\n",
       " 'interruption': 1,\n",
       " 'shout': 1,\n",
       " 'holiday': 1,\n",
       " 'commemoration': 1,\n",
       " 'responsible': 1,\n",
       " 'someone': 1,\n",
       " 'apologize': 1,\n",
       " 'struggle': 1,\n",
       " 'routine': 1,\n",
       " 'take': 1,\n",
       " 'intolerance': 1,\n",
       " 'awful': 1,\n",
       " 'even': 2,\n",
       " 'object': 1,\n",
       " 'good': 1,\n",
       " 'every': 1,\n",
       " 'conformity': 1,\n",
       " 'censor': 1,\n",
       " 'comfort': 1,\n",
       " 'need': 1,\n",
       " 'like': 1,\n",
       " 'death': 1,\n",
       " 'associate': 1,\n",
       " 'london': 1,\n",
       " 'repetition': 1,\n",
       " 'could': 1,\n",
       " 'end': 1,\n",
       " 'many': 1,\n",
       " 'mean': 2,\n",
       " 'much': 1,\n",
       " 'certainly': 1,\n",
       " 'look': 2,\n",
       " 'may': 1,\n",
       " 'longer': 1,\n",
       " 'happen': 1,\n",
       " 'soon': 1,\n",
       " 'saw': 1,\n",
       " 'day': 2,\n",
       " 'enunciation': 1,\n",
       " 'events': 1,\n",
       " 'oppression': 1,\n",
       " 'something': 1,\n",
       " 'wrong': 1,\n",
       " 'word': 2,\n",
       " 'surveillance': 1,\n",
       " 'think': 2,\n",
       " 'order': 1,\n",
       " 'gun': 1,\n",
       " 'bloke': 1,\n",
       " 'chat': 1,\n",
       " 'speak': 2,\n",
       " 'usually': 1,\n",
       " 'always': 1,\n",
       " 'tranquility': 1,\n",
       " 'truncheon': 1,\n",
       " 'daily': 1,\n",
       " 'spirit': 1,\n",
       " 'country': 1,\n",
       " 'lieu': 1,\n",
       " 'guilty': 1,\n",
       " 'submission': 1,\n",
       " 'nice': 1,\n",
       " 'live': 1,\n",
       " 'accountable': 1,\n",
       " 'course': 1,\n",
       " 'allow': 1,\n",
       " 'tell': 1,\n",
       " 'power': 1,\n",
       " '5th': 1,\n",
       " 'little': 1,\n",
       " 'time': 1,\n",
       " 'blame': 1,\n",
       " 'familiar': 1,\n",
       " 'solicit': 1,\n",
       " 'mirror': 1,\n",
       " 'truth': 3,\n",
       " 'appreciate': 1,\n",
       " 'us': 1,\n",
       " 'important': 1,\n",
       " 'enjoy': 1,\n",
       " 'retain': 1,\n",
       " 'freedom': 1,\n",
       " 'others': 1,\n",
       " 'fit': 1,\n",
       " 'thereby': 1,\n",
       " 'systems': 1,\n",
       " 'cruelty': 1,\n",
       " 'offer': 1}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{word: keywords.count(word) for word in set(keywords)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word ‘give’ occurs 3 times whereas ‘joyous’ was mentioned once.\n",
    "\n",
    "This is essentially what CountVectorizer does to all records. CountVectorizer transforms text into a matrix of m by n where m is the number of text records, n is the number of unique tokens across all records and the elements of the matrix refer to the tally of a token for a given record.\n",
    "\n",
    "In this step, we will convert our text dataframe to count matrix. We will pass our custom preprocessor function to CountVectorizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import module\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Create an instance of CountfVectorizer\n",
    "vectoriser = CountVectorizer(analyzer=preprocess_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe\n",
    "X_train = pd.DataFrame([part1, part2], columns=['speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit to the data and transform to feature matrix\n",
    "X_train = vectoriser.fit_transform(X_train['speech'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output feature matrix will be in sparse_matrix form. Let’s convert it to a dataframe with proper column names to make it more readible:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5th</th>\n",
       "      <th>accountable</th>\n",
       "      <th>adam</th>\n",
       "      <th>afraid</th>\n",
       "      <th>ago</th>\n",
       "      <th>allow</th>\n",
       "      <th>always</th>\n",
       "      <th>apologize</th>\n",
       "      <th>appreciate</th>\n",
       "      <th>ask</th>\n",
       "      <th>...</th>\n",
       "      <th>war</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wish</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   5th  accountable  adam  afraid  ago  allow  always  apologize  appreciate  \\\n",
       "0    1            1     0       0    0      1       1          1           1   \n",
       "1    3            0     1       1    1      1       0          0           0   \n",
       "\n",
       "   ask  ...  war  way  well  wish  word  world  would  wrong  year  years  \n",
       "0    0  ...    0    1     1     0     2      0      0      1     0      0  \n",
       "1    1  ...    1    0     0     1     1      1      2      0     1      1  \n",
       "\n",
       "[2 rows x 189 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert sparse matrix to dataframe\n",
    "X_train = pd.DataFrame.sparse.from_spmatrix(X_train)\n",
    "\n",
    "# Save mapping on which index refers to which terms\n",
    "col_map = {v:k for k, v in vectoriser.vocabulary_.items()}\n",
    "\n",
    "# Rename each column using the mapping\n",
    "for col in X_train.columns:\n",
    "    X_train.rename(columns={col: col_map[col]}, inplace=True)\n",
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we transform it to dataframe, the columns would be just indices (i.e. numbers from 0 to n-1) instead of the actual words. Therefore, we need to rename the columns to make it easier to interpret.\n",
    "\n",
    "When the vectoriser is fit to the data, we can find out the index mapping to words from vectoriser.vocabulary_. This index mapping is formatted as {word:index}. To rename columns, we must switch the key-value pairs to {index:word}. This is done in the second line of code and saved in col_map.\n",
    "\n",
    "Using for loop at the end of the code, we are renaming each column using the mapping, and the output should look like what is in the table above (showing only partial output due to space limitation).\n",
    "From this matrix, we can see ‘give’ has been mentioned 3 times in part1 (row index=0) and once in part2 (row index=1).\n",
    "\n",
    "In our example, we only have 2 records each consisting of only a handful of sentences, so the count matrix is pretty small and its sparsity is not as high. Sparsity refers to the proportion of zero elements among all elements in a matrix. When you are working on real data with hundreds, thousands or even millions of records each represented by rich text, the count matrix is likely to be extremely large and contain mostly 0s. \n",
    "\n",
    "In those instances, using sparse format saves storage memory and speeds up further processing. As a result, you may not always convert sparse matrix to a dataframe like we did here for illustration when preprocessing text in real life."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. Transform to TF-IDF Representation\n",
    "\n",
    "**tf-idf stands for term frequency inverse document frequency.**\n",
    "\n",
    "When transforming to tf-idf representation, we are transforming the counts to weighted frequency where we give more significance to less frequent words and less importance to more frequent words by using a weight called inverse document frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>5th</th>\n",
       "      <th>accountable</th>\n",
       "      <th>adam</th>\n",
       "      <th>afraid</th>\n",
       "      <th>ago</th>\n",
       "      <th>allow</th>\n",
       "      <th>always</th>\n",
       "      <th>apologize</th>\n",
       "      <th>appreciate</th>\n",
       "      <th>ask</th>\n",
       "      <th>...</th>\n",
       "      <th>war</th>\n",
       "      <th>way</th>\n",
       "      <th>well</th>\n",
       "      <th>wish</th>\n",
       "      <th>word</th>\n",
       "      <th>world</th>\n",
       "      <th>would</th>\n",
       "      <th>wrong</th>\n",
       "      <th>year</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.061335</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.122670</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086204</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.184983</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.061661</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>...</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.061661</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.173324</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.086662</td>\n",
       "      <td>0.086662</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 189 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        5th  accountable      adam    afraid       ago     allow    always  \\\n",
       "0  0.061335     0.086204  0.000000  0.000000  0.000000  0.061335  0.086204   \n",
       "1  0.184983     0.000000  0.086662  0.086662  0.086662  0.061661  0.000000   \n",
       "\n",
       "   apologize  appreciate       ask  ...       war       way      well  \\\n",
       "0   0.086204    0.086204  0.000000  ...  0.000000  0.086204  0.086204   \n",
       "1   0.000000    0.000000  0.086662  ...  0.086662  0.000000  0.000000   \n",
       "\n",
       "       wish      word     world     would     wrong      year     years  \n",
       "0  0.000000  0.122670  0.000000  0.000000  0.086204  0.000000  0.000000  \n",
       "1  0.086662  0.061661  0.086662  0.173324  0.000000  0.086662  0.086662  \n",
       "\n",
       "[2 rows x 189 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import module\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "# Create an instance of TfidfTransformer\n",
    "transformer = TfidfTransformer()\n",
    "\n",
    "# Fit to the data and transform to tf-idf\n",
    "X_train = pd.DataFrame(transformer.fit_transform(X_train).toarray(), columns=X_train.columns)\n",
    "X_train"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
